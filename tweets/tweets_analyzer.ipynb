{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline                  \n",
    "import matplotlib                   # make plots show up in notebook\n",
    "import time                         # time/date parser\n",
    "import csv                          # data parser\n",
    "import numpy as np                  # arrays for plotting\n",
    "import matplotlib.pyplot as plt     # plotting\n",
    "import math                         # ceiling for y-max in plots\n",
    "import twittercriteria as twc       # yaml, re, os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('../census/'))\n",
    "import bostonmap as bm\n",
    "sys.path.append(os.path.realpath('./classifiers/'))\n",
    "import tweetclassifier as tc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving useful data with our `twitter_criteria` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve the time format string from the criteria dictionary in twc\n",
    "tweet_time_fmt = twc.getTwitterTimeFmt()\n",
    "\n",
    "# build regex pattern for searching for all the keywords\n",
    "# Will look something like '#bostonmarathon|#marathonmonday|#patriotsday|marathon|...'\n",
    "keywords = twc.getKeywordRegex()\n",
    "\n",
    "# initialze hash/dictionary for counting keyword occurances\n",
    "keyword_counts = {}\n",
    "for word in twc.getKeywords():\n",
    "    keyword_counts[word] = {'frequency':0, 'tweets':0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def tweetContainsKeyWords(tweet):\n",
    "    # searches tweet for keywords, if none were found, findall returns empty list\n",
    "    found_words = keywords.findall(tweet)\n",
    "    if found_words:\n",
    "        for word in found_words:\n",
    "            keyword_counts[word]['frequency'] += 1\n",
    "        for word in set(found_words):\n",
    "            keyword_counts[word]['tweets'] += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def makePlot(dates, month):\n",
    "    ### produce subplots\n",
    "    ### source: http://people.duke.edu/~ccc14/pcfb/numpympl/MatplotlibBarPlots.html\n",
    "  \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.add_subplot(121) # 1x2 grid, 1st subplot\n",
    "\n",
    "    #### make data plot-friendly\n",
    "\n",
    "    bar_width = 0.35\n",
    "    indices = np.arange(len(dates))\n",
    "\n",
    "    ams = []\n",
    "    pms = []\n",
    "    tickMarks = []\n",
    "\n",
    "    # goes through the date hashes in order of day\n",
    "    for d in sorted(dates):\n",
    "        # adds the day's counts to each list respectively\n",
    "        ams.append(dates[d]['AM'])\n",
    "        pms.append(dates[d]['PM'])\n",
    "        # make string representing and add it to list\n",
    "        tickMarks.append(str(month) + '/' + str(d))\n",
    "\n",
    "    ##### get largest count of both lists and round up to nearest 100th\n",
    "    maxCount = int(math.ceil(max(ams + pms) / 100.0)) * 100\n",
    "\n",
    "    #### plot data\n",
    "\n",
    "    ##### first plot\n",
    "\n",
    "    amBars = ax.bar(indices, ams, bar_width, color='c')\n",
    "\n",
    "    pmBars = ax.bar(indices+bar_width, pms, bar_width, color='g')\n",
    "\n",
    "    ax.set_xlim(-bar_width, len(indices)+bar_width)\n",
    "    ax.set_ylim(0, maxCount)\n",
    "    ax.set_ylabel('Number of Tweets')\n",
    "    ax.set_title('Tweets Containing Keywords per Day', fontsize=10)\n",
    "    ax.set_xticks(indices+bar_width)\n",
    "    tickNames = ax.set_xticklabels(tickMarks)\n",
    "    plt.setp(tickNames, rotation=45, fontsize=10)\n",
    "\n",
    "    ax.legend((amBars[0], pmBars[0]), ('AM', 'PM'))\n",
    "\n",
    "    ##### second plot, keyword count\n",
    "    #source (simple way): http://stackoverflow.com/questions/17232683/creating-tables-in-matplotlib\n",
    "    \n",
    "    colLabs = ['Keyword', 'Frequency', '# Tweets']\n",
    "\n",
    "    table_data = []\n",
    "    for word in sorted(keyword_counts, key=keyword_counts.get, reverse=True):\n",
    "        table_data.append([word, keyword_counts[word]['frequency'], keyword_counts[word]['tweets']])\n",
    "    \n",
    "    ax2 = fig.add_subplot(122) # 1x2, 2nd subplot\n",
    "    ax2.axis('off') # makes blank subplot\n",
    "    \n",
    "    # add title and text\n",
    "    ax2.set_title('Frequencies of Keywords')\n",
    "    table = ax2.table(loc='center', colLabels=colLabs, cellText=table_data)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2,1.2)\n",
    "    \n",
    "    #### save plots as image\n",
    "    plt.savefig('tweets_per_day.png', dpi=96)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    ### get data\n",
    "\n",
    "    dates = {}\n",
    "    kwTweets = []\n",
    "    \n",
    "    tweetList = []\n",
    "    \n",
    "    textList= []\n",
    "\n",
    "    trainingPaths = {'relevant':'/relevantTraining.txt', 'irrelevant':'/irrelevantTraining.txt'}\n",
    "    \n",
    "    classifier = tc.TweetClassifierSVM(trainingPaths, twc.cleanUpTweet)\n",
    "    \n",
    "    with open('cleaned_geo_tweets_Apr_12_to_22.csv') as csvfile:\n",
    "        # reads first line of csv to determine keys for the tweet hash, tweets \n",
    "        # is an iterator through the list of tweet hashes the DictReader makes\n",
    "        tweets = csv.DictReader(csvfile)\n",
    "        # for all the tweets the reader finds\n",
    "        for tweetData in tweets:\n",
    "            # make sure its not a 'false tweet' from people using newlines in their tweet_text's\n",
    "            if tweetData['time'] != \"\":\n",
    "                # parse date/time into object\n",
    "                date = time.strptime(tweetData['time'], tweet_time_fmt)\n",
    "                # add day hash to list if it hasn't been added\n",
    "                if not date.tm_mday in dates.keys():\n",
    "                    dates[date.tm_mday] = {'AM':0, 'PM':0}\n",
    "                #tweetData['tweet_text'] = tweetData['tweet_text'].lower()\n",
    "                #if tweetContainsKeyWords(tweetData['tweet_text']):\n",
    "                #    kwTweets.append(tweetData)\n",
    "                #    # determine if morning or evening\n",
    "                #    if date.tm_hour < 12:                     # hour = 0 - 11\n",
    "                #        dates[date.tm_mday]['AM'] += 1\n",
    "                #    else:                                     # hour = 12 - 23\n",
    "                #        dates[date.tm_mday]['PM'] += 1\n",
    "                tweetList.append(tweetData)\n",
    "                textList.append(tweetData['tweet_text'])\n",
    "\n",
    "    results = classifier.classify(textList)\n",
    "    \n",
    "    for i in range(0, len(results)):\n",
    "        if results[i] == 0:\n",
    "            kwTweets.append(tweetList[i])\n",
    "            date = time.strptime(tweetList[i]['time'], tweet_time_fmt)\n",
    "            # determine if morning or evening\n",
    "            if date.tm_hour < 12:                     # hour = 0 - 11\n",
    "                dates[date.tm_mday]['AM'] += 1\n",
    "            else:                                     # hour = 12 - 23\n",
    "                dates[date.tm_mday]['PM'] += 1  \n",
    "                \n",
    "    makePlot(dates, date.tm_mon)\n",
    "    \n",
    "    del(dates)\n",
    "    del(date)\n",
    "    del(tweets)\n",
    "    del(tweetData)\n",
    "    \n",
    "    boston = bm.BostonScatter(kwTweets)\n",
    "    boston.plotMap(outname='bostonTweets', title='GeoTagged Tweets of 10-day Period Containing Keywords')\n",
    "      \n",
    "    boston = bm.GreaterBostonScatter(kwTweets)\n",
    "    boston.plotMap(outname='coastlineTweets', title='GeoTagged Tweets of 10-day Period Containing Keywords')\n",
    "    \n",
    "    boston = bm.BostonDensity(kwTweets)\n",
    "    del(kwTweets)\n",
    "    boston.plotMap(outname='densityTweets', title='Density of Geotagged Tweets of 10-day Period Containing Keywords')\n",
    "    \n",
    "    return\n",
    "\n",
    "## if this program is being executed, and not used as a module, call main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
