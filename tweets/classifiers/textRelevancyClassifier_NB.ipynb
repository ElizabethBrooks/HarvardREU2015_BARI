{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Text Classification ##\n",
    "\n",
    "Tha following program applies the Naive Bayes classifier provided by NLTK to input data files to determine the Twitter data set's relevancy to the Boston marathon bombing of 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Elizabeth Brooks\n",
    "# Date Modified: 07/08/2015\n",
    "\n",
    "# PreProcessor Directives\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "import csv\n",
    "import yaml\n",
    "import re\n",
    "from nltk.classify import apply_features\n",
    "import random\n",
    "# Directives for twc yaml\n",
    "import twittercriteria as twc\n",
    "twc.loadCriteria()\n",
    "keyword = twc.getKeywordRegex()\n",
    "twc.clearCriteria()\n",
    "\n",
    "# Global field declarations\n",
    "current_dir = os.getcwd()\n",
    "# Set the output file path\n",
    "resultsPath = current_dir + '/relevantTweetResults.txt'\n",
    "# Initialize the training and dev data sets\n",
    "trainSet, devSet, labeledTweets, featureSets = []\n",
    "\n",
    "# Function to clean up tweet strings \n",
    "# by manually removing irrelevant data (not words)\n",
    "def cleanUpTweet(tweet_text):\n",
    "    # Irrelevant characters\n",
    "    twitterMarkup = ['&amp;', 'http://t.co/']\n",
    "    temp = tweet_text.lower()\n",
    "    # Use regex to create a regular expression \n",
    "    # for removing undesired characters\n",
    "    temp = re.sub('|'.join(twitterMarkup), r\"\", temp)\n",
    "    return temp\n",
    "# End cleanUpTweet\n",
    "## The markup and the cleanUpTweet function will eventually \n",
    "## be moved to twc\n",
    "\n",
    "# Function to search for tweets based on pre-determined key words\n",
    "def tweetHasAKeyword(tweet_text):\n",
    "    return keyword.search(tweet_text) is not None\n",
    "# End tweetHasAKeyword\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a dictionary of relevent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to initialize the feature sets\n",
    "def initDictSet(class1_path, class2_path):\n",
    "    # Loop through the txt files line by line\n",
    "    # Assign labels to tweets\n",
    "    # Two classes, relevant and irrelevant to the marathon\n",
    "    with open(current_dir + class1_path, \"r\") as relevantFile:\n",
    "        for line in relevantFile:\n",
    "            for word in line.split():\n",
    "                labeledTweets.append(word, 'relevant')\n",
    "    with open(current_dir + class2_path, \"r\") as irrelevantFile:\n",
    "        for line in irrelevantFile:\n",
    "            for word in line.split():\n",
    "                labeledTweets.append(word, 'irrelevant')\n",
    "    # Randomize the data\n",
    "    random.shuffle(labeledTweets)\n",
    "    # Close the files\n",
    "    relevantTxtFile.close()\n",
    "    irrelevantTxtFile.close()\n",
    "# End initDictSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractFeatures(tweet_txt) function is used by the trainData() function to assign an input term to a feature set indicating marathon relevance. The feature set is then split into a training and test set. Then the training set is the used by the Naive Bayes classifier provided by NLTK to train the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract features from tweets\n",
    "def extractFeatures(train_file, test_file):\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + train_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Determine the feature sets\n",
    "            featureSets = [(tweetText, relevance) for (tweetText, relevance) in labeledTweets]\n",
    "        # End for\n",
    "    # End with\n",
    "    # Train the determined feature set\n",
    "    trainClassifyData(featureSets, test_file)\n",
    "# End extractFeatures\n",
    "\n",
    "# Function for training the classifier\n",
    "def trainClassifyData(feature_sets, test_file):   \n",
    "    # Establish the training and dev data sets\n",
    "    trainSet, devSet = feature_sets[500:], features_sets[:500] #before and after 500\n",
    "\n",
    "    # Train the Naive Bayes (NB) classifier\n",
    "    classifierNB = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "    \n",
    "    # Classify input test data\n",
    "    # Create object for writting to a text file\n",
    "    tweetResultsFile = open(resultsPath, \"w\")\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + test_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Send the results of the classifier to a txt file\n",
    "            tweetResultsFile.write(classifierNB.classify(tweetText))\n",
    "        # End for\n",
    "    # End with\n",
    "    # Close file\n",
    "    tweetResultsFile.close()\n",
    "# End trainClassifyData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method requests user input of class feature sets for Naive Bayes classification of tweets, as well as traing and test data sets of csv Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main method\n",
    "def main():\n",
    "    # Request user input of text class files\n",
    "    inputClassFile1 = 'relevantTraining.txt'\n",
    "    inputClassFile2 = 'irrelevantTraining.txt'\n",
    "\n",
    "    # Initialize the classifier dictionary based on relevant features\n",
    "    initDictSet(inputClassFile1, inputClassFile2)\n",
    "\n",
    "    # Request user input of the file name of train/dev data to be processed\n",
    "    inputTrainFile = raw_input(\"Enter train/dev data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    # Request file name of data to be classified\n",
    "    inputTestFile = raw_input(\"Enter test data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    \n",
    "    # Train the NB classifier using input tweet terms\n",
    "    extractFeatures(inputTrainFile, inputTestFile)\n",
    "# End main\n",
    "\n",
    "# Run the script via the main method\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# End script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
