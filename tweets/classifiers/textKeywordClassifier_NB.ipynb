{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Text Classification ##\n",
    "\n",
    "Tha following program applies the Naive Bayes classifier provided by NLTK to input data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Elizabeth Brooks\n",
    "# Date Modified: 07/06/2015\n",
    "\n",
    "# PreProcessor Directives\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "import csv\n",
    "import yaml\n",
    "import re\n",
    "from nltk.classify import apply_features\n",
    "import random\n",
    "# Directives for twc yaml\n",
    "import twittercriteria as twc\n",
    "twc.loadCriteria()\n",
    "twc.clearCriteria()\n",
    "\n",
    "# Global field declarations\n",
    "keyword = twc.getKeywordRegex()\n",
    "current_dir = os.getcwd()\n",
    "# Set the output file path\n",
    "relevantPath = current_dir + '/RelevantTweets.txt'\n",
    "irrelevantPath = current_dir + '/IrrelevantTweets.txt'\n",
    "# Initialize the training and dev data sets\n",
    "trainSet, devSet = []\n",
    "\n",
    "# Function to clean up tweet strings \n",
    "# by manually removing irrelevant data (not words)\n",
    "def cleanUpTweet(tweet_text):\n",
    "    # Irrelevant characters\n",
    "    twitterMarkup = ['&amp;', 'http://t.co/']\n",
    "    temp = tweet_text.lower()\n",
    "    # Use regex to create a regular expression \n",
    "    # for removing undesired characters\n",
    "    temp = re.sub('|'.join(twitterMarkup), r\"\", temp)\n",
    "    return temp\n",
    "# End cleanUpTweet\n",
    "\n",
    "# Function to search for tweets based on pre-determined key words\n",
    "def containsKeyword(tweet_text):\n",
    "    return keyword.search(tweet_text) is not None\n",
    "# End containsKeyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function removes irrelevant characters from the tweet strings contained in the Twitter data csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for organizing data for labeling\n",
    "def createRelevanceDict(txt_data):\n",
    "    # Create object for writting to a text file\n",
    "    relevantTxtFile = open(relevantPath, \"w\")\n",
    "    irrelevantTxtFile = open(irrelevantPath, \"w\")\n",
    "    # Write the tweets sorted to their respective files\n",
    "    with open(txt_data + \".csv\") as fileData:\n",
    "        for line in fileData:\n",
    "            # Determie the label to assign a tweet based on keyword\n",
    "            if containsKeyword(line):\n",
    "                relevantTxtFile.write(line + \"\\n\")\n",
    "            else:\n",
    "                irrelevantTxtFile.write(line + \"\\n\")\n",
    "            # End else\n",
    "    # Close the files\n",
    "    relevantTxtFile.close()\n",
    "    irrelevantTxtFile.close()\n",
    "    # Inititialize dictionary classes\n",
    "    initDictSet()\n",
    "# End createRelevanceDict\n",
    "\n",
    "# Function to initialize the feature sets\n",
    "def initDictSet():\n",
    "    # Create object for writting to a text file\n",
    "    relevantTxtFile = open(relevantPath, \"w\")\n",
    "    irrelevantTxtFile = open(irrelevantPath, \"w\")\n",
    "    # Assign labels to tweets\n",
    "    # Two classes, relevant and irrelevant to the marathon\n",
    "    labeledTweets = ([(word, 'relevant') for word in relevantTxtFile.read().split()] +\n",
    "        [(word, 'irrelevant') for word in irrelevantTxtFile.read().split()])\n",
    "    # Randomize the data\n",
    "    random.shuffle(labeledTweets)\n",
    "    # Close the files\n",
    "    relevantTxtFile.close()\n",
    "    irrelevantTxtFile.close()\n",
    "# End initDictSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions are used to first organize the data into text file by relevance accoring to the pre determined list of keyword, then a second function loads these tweets by word into a yaml to be used to build classes and identify tweets for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract features from tweets\n",
    "def extractFeatures(train_file, test_file):\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + train_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Determine the feature sets\n",
    "            featureSets = [tweetText, relevance) for (tweetText, relevance) in labeledTweets]\n",
    "        # End for\n",
    "    # End with\n",
    "    # Train the determined feature set\n",
    "    trainClassifyData(featureSets, test_file)\n",
    "# End extractFeatures\n",
    "\n",
    "# Function for training the classifier\n",
    "def trainClassifyData(feature_sets, test_file):   \n",
    "    # Establish the training and dev data sets\n",
    "    trainSet, devSet = feature_sets[500:], features_sets[:500] #before and after 500\n",
    "\n",
    "    # Train the Naive Bayes (NB) classifier\n",
    "    classifierNB = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "    \n",
    "    # Classify input test data\n",
    "    # Set the results file path\n",
    "    resultsFilePath = current_dir + '/ClassifierResults_NB.txt'\n",
    "    # Create object for writting to a text file\n",
    "    tweetResultsFile = open(resultsFilePath, \"w\")\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + test_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Send the results of the classifier to a txt file\n",
    "            tweetResultsFile.write(classifierNB.classify(tweetText))\n",
    "        # End for\n",
    "    # End with\n",
    "    # Close file\n",
    "    tweetResultsFile.close()\n",
    "# End trainClassifyData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractFeatures(tweet_txt) function is used by the trainData() function to assign an input term to a feature set indicating marathon relevance. The feature set is then split into a training and test set. Then the training set is the used by the Naive Bayes classifier provided by NLTK to train the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main method\n",
    "def main():\n",
    "    # Set the output file path\n",
    "    txtFilePath = current_dir + '/OutputTweets.txt'\n",
    "    # Create object for writting to a text file\n",
    "    tweetTxtFile = open(txtFilePath, \"w\")\n",
    "    \n",
    "    # Request user input of the file name of test data to be processed\n",
    "    inputFile = raw_input(\"Enter csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + inputFile + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve the strings of tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Convert tweets to lower case to pool words of the same spelling\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Write the selected Twitter data, tweets, to the txt file\n",
    "            tweetTxtFile.write(tweetText + \"\\n\")\n",
    "    # Close the file obj\n",
    "    tweetTxtFile.close()\n",
    "    # Organize the data by relevance according to keyword dictionary\n",
    "    createRelevanceDict(txtFilePath)\n",
    "    \n",
    "    # Request user input of the file name of train/dev data to be processed\n",
    "    inputTrainFile = raw_input(\"Enter train/dev data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    # Request file name of data to be classified\n",
    "    inputTestFile = raw_input(\"Enter test data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    \n",
    "    # Train the NB classifier using input tweet terms\n",
    "    extractFeatures(inputTrainFile, inputTestFile)\n",
    "# End main\n",
    "\n",
    "# Run the script via the main method\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# End script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
