{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Tweet Word Cloud (Mac OS) ##\n",
    "\n",
    "A program for generating tweet based word clouds. Specifically in regards to the _cleaned_, geo-tagged Twitter data from April 12 to 22 of 2013.\n",
    "\n",
    "The program may be ammended to create any number of word clouds given an input txt file. A csv file may be used as well, the relavent information from which is parsed and saved into a txt file for use by the word cloud function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Elizabeth Brooks\n",
    "# Date Modified: 06/25/2015\n",
    "# Edited: Hayden Fuss\n",
    "\n",
    "# PreProcessor Directives\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from scipy.misc import imread\n",
    "import re\n",
    "# Add parent directory to path for twc imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "import twitter_criteria as twc\n",
    "\n",
    "# Global field declarations\n",
    "twc.loadCriteria()\n",
    "keyword = twc.getKeywordRegex()\n",
    "twc.clearCriteria()\n",
    "\n",
    "# Function to clean up tweet strings \n",
    "# by manually removing irrelevant data (not words)\n",
    "def cleanUpTweet(tweet):\n",
    "    # Irrelevant characters\n",
    "    twitter_markup = ['&amp;', 'http://t.co/']\n",
    "    temp = tweet.lower()\n",
    "    # Use regex to create a regular expression \n",
    "    # for removing undesired characters\n",
    "    temp = re.sub('|'.join(twitter_markup), r\"\", temp)\n",
    "    return temp\n",
    "# End cleanUpTweet\n",
    "## The markup and the cleanUpTweet function will eventually \n",
    "## be moved to twc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function .cleanUpTweet(String) is used to remove irrelevant data, characters that are not words and are therefore not interesting. In order to produce a more refined word cloud, the .tweetHasAKeyword(String) searches the data set of cleaned tweet strings for those tweets containing our pre-determined set of key words (see below).\n",
    "\n",
    "keywords: [\"#bostonmarathon\",\n",
    "           \"#marathonmonday\",\n",
    "           \"#patriotsday\",\n",
    "           \"marathon\",\n",
    "           \"boylston\",\n",
    "           \"finish line\",\n",
    "           \"#bostonstrong\",\n",
    "           \"#bostonpride\",\n",
    "           \"#prayforboston\",\n",
    "           \"#pray4bos\",\n",
    "           \"bomb\",\n",
    "           \"explosion\",\n",
    "           \"explode\",\n",
    "           \"wounded\",\n",
    "           \"hostage\",\n",
    "           \"watertown\",\n",
    "           \"lockdown\",\n",
    "           \"manhunt\",\n",
    "           \"collier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for generating a word cloud\n",
    "## Mac OS specific font path in arguments\n",
    "def tweetWordCloud(file_path, mask_path=None, bg_color='white', num_words=150,\n",
    "                   font_path='/Library/Fonts/Microsoft Sans Serif.ttf', out_path='keywordCloud.png'):\n",
    "    # Read in the txt file set by the main method\n",
    "    text = open(file_path, 'r').read()\n",
    "    # Generate the word cloud based on the input arguments\n",
    "    wc = None\n",
    "    if mask_path:\n",
    "        # Use module to read the image file\n",
    "        mass_mask = imread(mask_path)\n",
    "        wc = WordCloud(maskPath=mass_mask, backgroundColor=bg_color, maxWords=num_words, \n",
    "                       fontPath=font_path).generate(text)\n",
    "    else:\n",
    "        wc = WordCloud(fontPath=font_path, maxWords=num_words,\n",
    "                       backgroundColor=bg_color).generate(text)\n",
    "    # Open a plot of the generated word cloud\n",
    "    wc.to_file(out_path)\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "# End tweetWordCloud\n",
    "## We can eventually move this to its own module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function generates a word cloud based on words in the txt file created in the main method.\n",
    "For more info see: https://github.com/amueller/word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main method\n",
    "def main():\n",
    "    # Set the output image file path\n",
    "    cd = os.getcwd()\n",
    "    filepath = cd + '/OutputTweets.txt'\n",
    "    # Create object for writting to a text file\n",
    "    tweetFile = open(filepath, \"w\")\n",
    "    # Iterate through the \"cleaned\" Twitter data by tweet\n",
    "    with open(os.getcwd() + '/../cleaned_geo_tweets_Apr_12_to_22.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve the strings of tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Use function to clean tweet strings\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "    # Close the file obj\n",
    "    tweetFile.close()  \n",
    "    # Use the defined function to create the tweet word cloud\n",
    "    tweetWordCloud(filepath, cd)\n",
    "# End main\n",
    "\n",
    "# Run the script via the main method\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# End script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a file object and opens a txt file to store all the strings of tweets contained in the Twitter data from the \"cleaned\" tweets csv file. The tweet strings are then retrieved from the csv file by iterating through the tweet hashes made by the DictReader. Finally, these strings are converted to lower case. This is because we are interested in looking at the occurances of a given spelling of a word, regardless of case; stemming may be considered in the future to combine the frequencies of all conjugate forms of a words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
